{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "In many practical applications, although the data reside in a high-dimensional space, the true dimensionality, known as intrinsic dimensionality, can be of a much lower value.\n",
    "\n",
    " For example, in a three-dimensional space, the data may cluster around a straight line, or around the circumference of a circle or the graph of a parabola, arbitrarily placed in R^3. In all previous cases, the intrinsic dimensionality of the data is equal to one, as any of these curves can equivalently\n",
    "be described in terms of a single parameter.\n",
    "\n",
    "Below figure illustrates the three cases. Learning the lower-dimensional structure associated with a given set of data is gaining in importance in the context of big data processing and analysis. Some typical examples are the disciplines of computer vision, robotics, medical imaging, and computational neuroscience.\n",
    "\n",
    "![img](https://i.imgur.com/IkFGd65.png)\n",
    "\n",
    "The data reside close to (A) a straight line, (B) the circumference of a circle, and (C) the graph of a parabola in the three-dimensional space. In all three cases, the intrinsic dimensionality of the data is equal to one. In (A) the data are clustered around a (translated/affine) linear subspace and in (B) and (C) around one-dimensional manifolds.\n",
    "\n",
    "#### Some notes on Terminologies used here - When I say x_i belongs to R^d that means x_i is a d-dimensional column vector\n",
    "\n",
    "![img](https://i.imgur.com/ppkf04S.png)\n",
    "\n",
    "Which is a d * 1 Matrix\n",
    "\n",
    "So in the case of this famous [iris dataset](https://www.kaggle.com/arshid/iris-flower-dataset?select=IRIS.csv) x_i belongs to R^4 as it has 4-features and is a 4-dimensional dataset.\n",
    "\n",
    "![img](https://i.imgur.com/GifMvTp.png)\n",
    "\n",
    "#### The various methods used for dimensionality reduction include:\n",
    "\n",
    "- Principal Component Analysis (PCA)\n",
    "\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "\n",
    "- Generalized Discriminant Analysis (GDA)\n",
    "\n",
    "Dimensionality reduction may be both linear or non-linear, depending upon the method used. The prime linear method, called Principal Component Analysis, or PCA, is discussed below.\n",
    "\n",
    "\n",
    "\n",
    "## When should I use Dimensionality reduction\n",
    "\n",
    "Think of PCA as an exploratory technique to investigate and study your system before doing other things. Dimensionality reduction, by nature, loses some information, just like image compression for instance. Consequently, it will often reduce the prediction quality of your model, but depending on the data it could also leave it unchanged or even improve it in some cases (very noisy data). In general, its main benefit will be to speed up training.\n",
    "\n",
    "Besides speed of training, feature reduction also help with multicollinearity in some cases. Which is mainly an issue if you are interested in parameters estimation, like in causal analysis. For instance, if you use a multiple linear regression model to estimate the effect of some regressors on a dependent variable, multicollinearity will prevent proper parameters estimation because you won't be able to identify the effect of each regressor.\n",
    "\n",
    "However, in many other ML applications, we don't really care about parameters identification, we care about how a set of variables can be used to predict another variable. If some variables are highly correlated, then there will be some redundant information in your data, but this shouldn't be problematic in terms of prediction quality.\n",
    "\n",
    "In my experience, in couple of Kaggle Challenges, after doing some PCA, the accuracy has actually went down. After googling found out that some infer that PCA assumed that the variance could translate to high differentiable information which classifier could use. But at the same time, it threw some information away(not just noise), and these information could play a huge role in classification, although the variance is low.\n",
    "\n",
    "---\n",
    "\n",
    "## Data preparation for dimensionality reduction\n",
    "\n",
    "There is no best technique for dimensionality reduction and no mapping of techniques to problems.\n",
    "\n",
    "Instead, the best approach is to use systematic controlled experiments to discover what dimensionality reduction techniques, when paired with your model of choice, result in the best performance on your dataset.\n",
    "\n",
    "Typically, linear algebra and manifold learning methods assume that all input features have the same scale or distribution. This suggests that it is good practice to either normalize or standardize data prior to using these methods if the input variables have differing scales or units.\n",
    "\n",
    "## Comparison of feature selection, PCA, and clustering as dimension reduction schemes\n",
    "\n",
    "![img](https://i.imgur.com/1KUD3gN.png)\n",
    "\n",
    "The first 2 methods, Feature Selection and PCA reduce the dimension of the feature space, or in other words the number of rows in a data matrix. However, the two methods work differently: while feature selection literally selects rows from the original matrix to keep, PCA uses the geometry of the feature space to produce a new data matrix based on a lower feature dimensional version of the data. K-means, on the other hand, reduces the dimension of the data/number of data points, or equivalently the number of columns in the input data matrix. It does so by finding a small number of new averaged representatives or “centroids” of the input data, forming a new data matrix whose fewer columns (which are not\n",
    "present in the original data matrix) are precisely these centroids.\n",
    "\n",
    "## Mean Vector\n",
    "\n",
    "The first step in analyzing multivariate data is computing the mean vector and the variance-covariance matrix.\n",
    "\n",
    "The sample mean is a vector each of whose elements is the sample mean of one of the random variables – that is, each of whose elements is the arithmetic average of the observed values of one of the variables.\n",
    "\n",
    "![img](https://i.imgur.com/XG2Yag7.png)\n",
    "\n",
    "For example, take the following two vectors\n",
    "\n",
    "x1 = [2.2, 4.2, …]\n",
    "\n",
    "x2 = [1.2, 3.2, …]\n",
    "\n",
    "So the formulae for calculating `x_mean = 1/2(x1+x2)`\n",
    "\n",
    "= 0.5* [3.4, 7.4, ..]\n",
    "\n",
    "= [1.7,3.7, .. ]\n",
    "\n",
    "#### So we just summed up elements at ith index of the first array and the corresponding index of the second array.\n",
    "\n",
    "#### That also means, that we can say every array can be considered as a vector with each of its indices as one of the dimensions.\n",
    "\n",
    "## Covariance Matrix\n",
    "\n",
    "Covariances Matrix consists of covariances between each pair of variables in the other matrix positions.\n",
    "\n",
    "First a quick look at the difference between covariance and variance. Variance measures the variation of a single random variable (like the height of a person in a population), whereas covariance is a measure of how much two random variables vary together (like the height of a person and the weight of a person in a population). The formula for variance is given by\n",
    "\n",
    "![img](https://i.imgur.com/CnnSCxv.png)\n",
    "\n",
    "### Definition of Co-Variance for 2 scaler values from [Wikipedia](https://en.wikipedia.org/wiki/Covariance)\n",
    "\n",
    "For two jointly distributed real-valued random variables X and Y with finite second moments, the covariance is defined as the expected value (or mean) of the product of their deviations from their individual expected values\n",
    "\n",
    "\n",
    "![img](https://i.imgur.com/jE6qkN7.png)\n",
    "\n",
    "where E[X] is the expected value of X, also known as the mean of X.\n",
    "\n",
    "\n",
    "### Definition of Co-Variance for Matrix form of data - i.e. where the X (as defined in the above formulae is not a Scaler but is a Vector )\n",
    "\n",
    "![img](https://i.imgur.com/eS35C7X.png)\n",
    "\n",
    "#### and  very importantly the transpose appears in this case (for the Matrix form ) because we are multiplying two vectors.\n",
    "\n",
    "#### In another form the formulae for the Covariance Matrix is\n",
    "\n",
    "![img](https://i.imgur.com/AZ10i59.png)\n",
    "\n",
    "Stressing the last point from above if the columns of the original Matrix are standardized, meaning making the column vectors such that the mean is zero and Standard Deviation is 1 the above formulate for Covariance of a Matrix becomes simply\n",
    "\n",
    "#### XX^T/(n-1)\n",
    "\n",
    "## Matrix Multiplication Basics - Explanation on why we are using the transpose of the second element in calculating Covariance Matrix\n",
    "\n",
    "### Rules of matrix multiplication\n",
    "\n",
    "#### a⋅b = a^Tb\n",
    "\n",
    "On the other hand, **b⋅a≠baT**, which is an n×n matrix. The correct expression is\n",
    "\n",
    "#### b⋅a = b^Ta\n",
    "\n",
    "![img](https://i.imgur.com/ZrW2xUy.png)\n",
    "\n",
    "We are indeed taking dot products here. Note, though, that a is a column vector, but a^T is a row vector.\n",
    "\n",
    "By the rules of matrix multiplication, a^Ta and a^Tb result in a 1×1 matrix, which is equivalent to a scalar, while aa^T produces an n×n matrix:\n",
    "\n",
    "![img](https://i.imgur.com/6xjyH0y.png)\n",
    "\n",
    "[Source](https://math.stackexchange.com/a/1853890/517433)\n",
    "\n",
    "Lets see a simple example of a 2*1 Matrix to understand the above vector multiplications and using the Transformed vector for the second element in the above formulae.\n",
    "\n",
    "![img](https://i.imgur.com/KVAIo6b.png)\n",
    "\n",
    "#### [To Read further on Dot product in matrix notation](https://mathinsight.org/dot_product_matrix_notation)\n",
    "\n",
    "![img](https://i.imgur.com/sIBKvlv.png)\n",
    "\n",
    "\n",
    "### Note on using the unbiased estimator (1/n-1) instead of (1/n)\n",
    "\n",
    "This is termed as Bessel's correction. Think of it this way. When we calculate the sample standard deviation from a sample of n values, we are using the sample mean already calculated from that same sample of n values. The calculated sample mean has already \"used up\" one of the \"degrees of freedom of variability\"(which is the mean itself) that is available in the sample. Only n-1 degrees of freedom of variability are left for the calculation of the sample standard deviation.\n",
    "\n",
    "For further details on Bessel's Correction read the following\n",
    "\n",
    "- http://vortex.ihrc.fiu.edu/MET4570/members/Lectures/Lect05/m10divideby_nminus1.pdf\n",
    "- https://math.stackexchange.com/questions/61251/intuitive-explanation-of-bessels-correction\n",
    "\n",
    "---\n",
    "\n",
    "Let’s start with the below matrix.\n",
    "\n",
    "```python\n",
    "A = [[1, 3, 5],\n",
    "       [5, 4, 1],\n",
    "       [3, 8, 6]]\n",
    "```\n",
    "\n",
    "That is our original dataframe is of the below form.\n",
    "\n",
    "![img](https://i.imgur.com/UMS11Dc.png)\n",
    "\n",
    "### We will calculate the covariance between the first and the third column vectors (x1 and x3 features):\n",
    "\n",
    "![img](https://i.imgur.com/J2A8cos.png)\n",
    "\n",
    "![img](https://i.imgur.com/BZbzA1m.png[/img])\n",
    "\n",
    "So, that’s the value (-2.67 ) of the covariance matrix.\n",
    "\n",
    "Now use numpy to do build the full Covariance Matrix. Importantly, here I want NumPy to use the columns as vectors, so the parameter `rowvar=False` has to be used. Also, `bias=True` divides by n and not by n-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3 5]\n",
      " [5 4 1]\n",
      " [3 8 6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.66666667,  0.66666667, -2.66666667],\n",
       "       [ 0.66666667,  4.66666667,  2.33333333],\n",
       "       [-2.66666667,  2.33333333,  4.66666667]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[1, 3, 5], [5, 4, 1], [3, 8, 6]])\n",
    "# A = np.array([[ 3, 5], [4, 1], [8, 6]])\n",
    "print(A)\n",
    "\n",
    "# array([[1, 3, 5],\n",
    "#        [5, 4, 1],\n",
    "#        [3, 8, 6]])\n",
    "\n",
    "cov_matrix = np.cov(A, rowvar=False, bias=True )\n",
    "cov_matrix\n",
    "\n",
    "# array([[ 2.66666667, 0.66666667, -2.66666667],\n",
    "#        [ 0.66666667, 4.66666667, 2.33333333],\n",
    "#        [-2.66666667, 2.33333333, 4.66666667]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### The diagonal elements of this matrix are the variances of the variables, and the off-diagonal elements are the covariances between the variables.\n",
    "\n",
    "In below image all green highlighted are the variances and the rest (red highlighted) are the co-variances\n",
    "\n",
    "![img](https://i.imgur.com/8PJ5lMV.png)\n",
    "\n",
    "### Note, the Dimension of the Co-Variance Matrix will always be a Square Matrix -\n",
    "\n",
    "A Square Matrix is one with same number of rows and columns). Because they capture the covariance between any Variable and any other Variable. So we need all the Variables (Features in a dataset) on the columns and also all the variables on the rows in the resultant Co-Variance Matrix. Thats why in the above image I have put all of x1, x2, x3 in both the columns and rows.\n",
    "\n",
    "#### So, m-dimensional data will result in mxm covariance matrix\n",
    "\n",
    "Basically for three variables (or 3 dimensions of a dataset or 3 features of a dataset) X, Y and Z we will get the Covariance Matrix as following\n",
    "\n",
    "![img](https://i.imgur.com/AAq7ZC1.png)\n",
    "\n",
    "![img](https://i.imgur.com/j0vnWlR.png)\n",
    "\n",
    "Thus,\n",
    "\n",
    "- 2.66666667 is the variance of the x1 variable,\n",
    "\n",
    "- 0.66666667 is the covariance between x1 and x2 variables,\n",
    "\n",
    "- -2.66666667 is the covariance between x1 and x3 variables,\n",
    "\n",
    "- 4.66666667 is the variance of x2,\n",
    "\n",
    "- 2.33333333 is the covariance between x2 and x3 and\n",
    "\n",
    "- 4.66666667 is the variance of x3.\n",
    "\n",
    "#### The mean vector is often referred to as the centroid and the variance-covariance matrix as the dispersion or dispersion matrix. Also, the terms variance-covariance matrix and covariance matrix are used interchangeably.\n",
    "\n",
    "## Properties of Co-Variance Matrix\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Principal Component Analysis\n",
    "\n",
    "PCA stands for Principal Component Analysis. Here is what Wikipedia says about PCA.\n",
    "\n",
    "\"Given a collection of points in two, three, or higher dimensional space, a “best fitting” line can be defined as one that minimizes the average squared distance from a point to the line. The next best-fitting line can be similarly chosen from directions perpendicular to the first. Repeating this process yields an orthogonal basis in which different individual dimensions of the data are uncorrelated. These basis vectors are called principal components, and several related procedures principal component analysis (PCA).\"\n",
    "\n",
    "So, PCA means extracting the components (features) which are most effective in forecasting the target variable in a dataset, and discarding the redundant features. Thus, we calculate the Principal Components to achieve Dimensionality reduction.\n",
    "\n",
    "PCA ( aka Karhunen–Loève transform in Mathematics ) is among the oldest and most widely used methods for dimensionality reduction [105]. The assumption underlying PCA, as well as any dimensionality reduction technique, is that the observed data are generated by a system or process that is driven by a (relatively) small number of latent (not directly observed) variables. The goal is to learn this latent structure.\n",
    "\n",
    "Here's a visual representation of PCA being applied to a simple 2-D data set (left, in blue)\n",
    "\n",
    "![img](https://i.imgur.com/lKK6Gts.png)\n",
    "\n",
    "As we can see above dimension reduction via PCA retains much of the structure of the original data. The ideal subspace is shown in black in the left panel, and the data projected onto this subspace is shown in blue on the right. (bottom panels)\n",
    "\n",
    "Before using PCA, we must standardize our dataset. The method won’t work if we have entirely different scales for our data. Standardization helps to avoid biased outcomes.\n",
    "\n",
    "For example, suppose there are three variables – v1 in a 1-10 range, v2 in a 10-100 range, and v3 in a 100,000 to 1,000,000 range. If we just go ahead and compute an output using these predictors, we’ll get a heavily biased result because the third variable will have a disproportionately large impact on the output value.\n",
    "\n",
    "#### Step-1 So, Before applying PCA, we must ensure that all our attributes (dimensions) are centered around zero and have a standard deviation of 1. So, the first thing we do is calculate the means for all of them. Then, we center the values in each column (to do so we subtract the mean column value) and calculate the covariance matrix for the resulting centered matrix.\n",
    "\n",
    "Now, we can calculate our first principal component.\n",
    "\n",
    "#### Step-2 - Next, we utilize one of the methods for breaking up matrices (eigendecomposition or singular value decomposition) to turn our matrix into a list of eigenvectors that will define directions of axes in the new subspace and eigenvalues that will show the magnitude of those directions.\n",
    "\n",
    "We sort the vectors by their eigenvalues in decreasing order and thus get a ranking of the principal components.\n",
    "\n",
    "#### Step-3 - Now finally, we can ditch some of the dimensions and project our dataset into a reduced subspace without losing important information about the original dataset. And since new axes capture the maximum variance in the data, any clustering algorithm we’re going to apply next will have an easier time grouping the data instances.\n",
    "\n",
    "Lets look at it visually,\n",
    "\n",
    "Imagine this is our dataset that we’re trying to cluster; we only have two dimensions.\n",
    "\n",
    "![img](https://i.imgur.com/CUWkBzi.png)\n",
    "\n",
    "If we project the data onto the horizontal axis (our attribute 1) we won’t see much spread; it will show a nearly equal distribution of the observations.\n",
    "\n",
    "![img](https://i.imgur.com/s5O8hYn.png)\n",
    "\n",
    "Attribute 2, obviously, isn’t hugely helpful either.\n",
    "\n",
    "![img](https://i.imgur.com/6VnyFk7.png)\n",
    "\n",
    "As the data points in this case are spreading diagonally so we need a new line that would better capture this.\n",
    "\n",
    "![img](https://i.imgur.com/B9MtmIg.png)\n",
    "\n",
    "#### This axis is our first Principle Component (PC) – a new direction (we can imagine it to be an attribute) that maximizes the variance of the data (and thus the clusters become much more obvious.) Besides maximizing the spread, this first PC sits through the direction in the data that minimizes the distances between all the points and the new axis.\n",
    "\n",
    "#### The second PC must represent the second maximum amount of variance; it’s going to be a line that’s orthogonal to our first axis.\n",
    "\n",
    "![img](https://i.imgur.com/ocDf36C.png)\n",
    "\n",
    "Because of the underlying PCA-Math being based on eigenvectors and eigenvalues, new principal components will always come out orthogonal to the ones before them. **Eigenvectors**, simply, are vectors that aren’t knocked off of their span by a linear transformation; they can hold on to their original direction while being stretched, shrunk or reversed. Eigenvalues are factors by which these special vectors are scaled.\n",
    "\n",
    "\n",
    "\n",
    "## Key concept of Preserving the Variance by Projection of Data\n",
    "\n",
    "#### Before we can project the training set onto a lower-dimensional hyperplane, we first need to choose the right hyperplane. For example, in the below figure, a simple 2D dataset is represented on the left, along with three different axes (i.e., 1D hyperplanes). On the right is the result of the projection of the dataset onto each of these axes. As we can see, the projection onto the solid line preserves the maximum variance, while the projection onto the dotted line preserves very little variance and the projection onto the dashed line preserves an intermediate amount of variance.\n",
    "\n",
    "![img](https://i.imgur.com/Fp8kqgJ.png)\n",
    "\n",
    "#### It seems reasonable to select the axis that preserves the maximum amount of variance, as it will most likely lose less information than the other projections. Another way to justify this choice is that it is the axis that minimizes the mean squared distance between the original dataset and its projection onto that axis. This is the basic idea behind PCA.\n",
    "\n",
    "PCA identifies the axis that accounts for the largest amount of variance in the train-ing set. In above figure, it is the solid line. It also finds a second axis, orthogonal to the first one, that accounts for the largest amount of remaining variance. In this 2D example there is no choice: it is the dotted line (i.e. the orthogonal one). If it were a higher-dimensional data‐set, PCA would also find a third axis, orthogonal to both previous axes, and a fourth, a fifth, and so on—as many axes as the number of dimensions in the dataset. The ith axis is called the ith principal component (PC) of the data. In above figure, the first PC is the axis on which vector c1 lies, and the second PC is the axis on which vector c2 lies. In case of 3-D dataset the first two PCs are the orthogonal axes on which the two arrows lie, on the plane, and the third PC is the axis orthogonal to that plane, like in the below figure.\n",
    "\n",
    "![img](https://i.imgur.com/wEHwUMl.png)\n",
    "\n",
    "\n",
    "For each principal component, PCA finds a zero-centered unit vector pointing in the direction of the PC. Since two opposing unit vectors lie on the same axis, the direction of the unit vectors returned by PCA is not stable: if you perturb the training set slightly and run PCA again, the unit vectors may point in the oppo‐\n",
    "site direction as the original vectors. However, they will generally still lie on the same axes. In some cases, a pair of unit vectors may even rotate or swap (if the variances along these two axes are close), but the plane they define will generally remain the same.\n",
    "\n",
    "\n",
    "## The Mathematics of PCA\n",
    "\n",
    "### Under the Maximum Variance Formulae\n",
    "\n",
    "### First here is the what the Projection Vector looks like geometrically, for point x_i and also the mathematical formula of point x_i on the Principle Component Vector u_1\n",
    "\n",
    "![img](https://i.imgur.com/WirAvCo.png)\n",
    "\n",
    "In English the here the Math Objective of the Problem - assuming **u1** represents the Principle component Vector\n",
    "\n",
    "### Find u1, such that the the Variance of points x_i ( i going from 1 to n) projected onto u1 is maximized\n",
    "\n",
    "Mathematically\n",
    "\n",
    "![img](https://i.imgur.com/E5mMcaD.png)\n",
    "\n",
    "A slightly more formally\n",
    "\n",
    "![img](https://i.imgur.com/teprtEQ.png)\n",
    "\n",
    "[Source of above Image](https://www.cs.toronto.edu/~urtasun/courses/CSC411/tutorial8.pdf)\n",
    "\n",
    "#### But note our original dataset Matrix has been centered or column-standardized, hence\n",
    "\n",
    "![img](https://i.imgur.com/yAj6dE6.png)\n",
    "\n",
    "### So the final Objective function becomes\n",
    "\n",
    "![img](https://i.imgur.com/aBMJhVk.png)\n",
    "\n",
    "---\n",
    "\n",
    "Don’t confuse PCA with linear regression. PCA is to minimize the distance of the orthogonal projection, while linear regression is to minimize the distance on the y-axis.\n",
    "\n",
    "![img](https://i.imgur.com/x3cFb0o.png)\n",
    "\n",
    "[Source of above image](http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/)\n",
    "\n",
    "## Why is minimizing squared residuals equivalent to maximizing variance?\n",
    "\n",
    "To ans this question I could not find a better explanations than has been describe **[here in this blog](http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/)**\n",
    "\n",
    "![img](https://i.imgur.com/w7gM6lu.png)\n",
    "\n",
    " Where **w** is the projection of each datapoint onto the top principal component and the vector **c**  is the top principal component\n",
    "\n",
    "#### Important Note - PCA assumes that the dataset is centered around the origin. As we will see, Scikit-Learn’s PCA classes take care of centering the data for you. If you implement PCA yourself, or if you use other libraries, don’t forget to center the data first"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}