{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In many practical applications, although the data reside in a high-dimensional space, the true dimen-\n",
    "sionality, known as intrinsic dimensionality, can be of a much lower value.\n",
    "\n",
    " For example, in a three-dimensional space, the data may cluster around a straight line, or around the circumference of a circle or the graph of a parabola, arbitrarily placed in R^3. In all previous cases, the intrinsic dimensionality of the data is equal to one, as any of these curves can equivalently\n",
    "be described in terms of a single parameter.\n",
    "\n",
    "Below figure illustrates the three cases. Learning the lower-dimensional structure associated with a given set of data is gaining in importance in the context of big data processing and analysis. Some typical examples are the disciplines of computer vision, robotics, medical imaging, and computational neuroscience.\n",
    "\n",
    "![img](https://i.imgur.com/IkFGd65.png)\n",
    "\n",
    "The data reside close to (A) a straight line, (B) the circumference of a circle, and (C) the graph of a parabola in the three-dimensional space. In all three cases, the intrinsic dimensionality of the data is equal to one. In (A) the data are clustered around a (translated/affine) linear subspace and in (B) and (C) around one-dimensional manifolds.\n",
    "\n",
    "\n",
    "#### The various methods used for dimensionality reduction include:\n",
    "\n",
    "- Principal Component Analysis (PCA)\n",
    "\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "\n",
    "- Generalized Discriminant Analysis (GDA)\n",
    "\n",
    "Dimensionality reduction may be both linear or non-linear, depending upon the method used. The prime linear method, called Principal Component Analysis, or PCA, is discussed below.\n",
    "\n",
    "## PRINCIPAL COMPONENT ANALYSIS\n",
    "\n",
    "Principal component analysis (PCA) or Karhunen–Loève transform is among the oldest and most widely used methods for dimensionality reduction [105]. The assumption underlying PCA, as well as any dimensionality reduction technique, is that the observed data are generated by a system or process\n",
    "that is driven by a (relatively) small number of latent (not directly observed) variables. The goal is to learn this latent structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}