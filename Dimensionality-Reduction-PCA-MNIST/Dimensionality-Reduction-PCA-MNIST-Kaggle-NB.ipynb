{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "In many practical applications, although the data reside in a high-dimensional space, the true dimensionality, known as intrinsic dimensionality, can be of a much lower value.\n",
    "\n",
    " For example, in a three-dimensional space, the data may cluster around a straight line, or around the circumference of a circle or the graph of a parabola, arbitrarily placed in R^3. In all previous cases, the intrinsic dimensionality of the data is equal to one, as any of these curves can equivalently\n",
    "be described in terms of a single parameter.\n",
    "\n",
    "Below figure illustrates the three cases. Learning the lower-dimensional structure associated with a given set of data is gaining in importance in the context of big data processing and analysis. Some typical examples are the disciplines of computer vision, robotics, medical imaging, and computational neuroscience.\n",
    "\n",
    "![img](https://i.imgur.com/IkFGd65.png)\n",
    "\n",
    "The data reside close to (A) a straight line, (B) the circumference of a circle, and (C) the graph of a parabola in the three-dimensional space. In all three cases, the intrinsic dimensionality of the data is equal to one. In (A) the data are clustered around a (translated/affine) linear subspace and in (B) and (C) around one-dimensional manifolds.\n",
    "\n",
    "#### Some notes on Terminologies used here - When I say x_i belongs to R^d that means x_i is a d-dimensional column vector\n",
    "\n",
    "![img](https://i.imgur.com/ppkf04S.png)\n",
    "\n",
    "Which is a d * 1 Matrix\n",
    "\n",
    "So in the case of this famous [iris dataset](https://www.kaggle.com/arshid/iris-flower-dataset?select=IRIS.csv) x_i belongs to R^4 as it has 4-features and is a 4-dimensional dataset.\n",
    "\n",
    "![img](https://i.imgur.com/GifMvTp.png)\n",
    "\n",
    "#### The various methods used for dimensionality reduction include:\n",
    "\n",
    "- Principal Component Analysis (PCA)\n",
    "\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "\n",
    "- Generalized Discriminant Analysis (GDA)\n",
    "\n",
    "Dimensionality reduction may be both linear or non-linear, depending upon the method used. The prime linear method, called Principal Component Analysis, or PCA, is discussed below.\n",
    "\n",
    "\n",
    "\n",
    "## When should I use Dimensionality reduction\n",
    "\n",
    "Think of PCA as an exploratory technique to investigate and study your system before doing other things. Dimensionality reduction, by nature, loses some information, just like image compression for instance. Consequently, it will often reduce the prediction quality of your model, but depending on the data it could also leave it unchanged or even improve it in some cases (very noisy data). In general, its main benefit will be to speed up training.\n",
    "\n",
    "Besides speed of training, feature reduction also help with multicollinearity in some cases. Which is mainly an issue if you are interested in parameters estimation, like in causal analysis. For instance, if you use a multiple linear regression model to estimate the effect of some regressors on a dependent variable, multicollinearity will prevent proper parameters estimation because you won't be able to identify the effect of each regressor.\n",
    "\n",
    "However, in many other ML applications, we don't really care about parameters identification, we care about how a set of variables can be used to predict another variable. If some variables are highly correlated, then there will be some redundant information in your data, but this shouldn't be problematic in terms of prediction quality.\n",
    "\n",
    "In my experience, in couple of Kaggle Challenges, after doing some PCA, the accuracy has actually went down. After googling found out that some infer that PCA assumed that the variance could translate to high differentiable information which classifier could use. But at the same time, it threw some information away(not just noise), and these information could play a huge role in classification, although the variance is low.\n",
    "\n",
    "---\n",
    "\n",
    "## Data preparation for dimensionality reduction\n",
    "\n",
    "There is no best technique for dimensionality reduction and no mapping of techniques to problems.\n",
    "\n",
    "Instead, the best approach is to use systematic controlled experiments to discover what dimensionality reduction techniques, when paired with your model of choice, result in the best performance on your dataset.\n",
    "\n",
    "Typically, linear algebra and manifold learning methods assume that all input features have the same scale or distribution. This suggests that it is good practice to either normalize or standardize data prior to using these methods if the input variables have differing scales or units.\n",
    "\n",
    "## Comparison of feature selection, PCA, and clustering as dimension reduction schemes\n",
    "\n",
    "![img](https://i.imgur.com/1KUD3gN.png)\n",
    "\n",
    "The first 2 methods, Feature Selection and PCA reduce the dimension of the feature space, or in other words the number of rows in a data matrix. However, the two methods work differently: while feature selection literally selects rows from the original matrix to keep, PCA uses the geometry of the feature space to produce a new data matrix based on a lower feature dimensional version of the data. K-means, on the other hand, reduces the dimension of the data/number of data points, or equivalently the number of columns in the input data matrix. It does so by finding a small number of new averaged representatives or “centroids” of the input data, forming a new data matrix whose fewer columns (which are not\n",
    "present in the original data matrix) are precisely these centroids.\n",
    "\n",
    "## Mean Vector\n",
    "\n",
    "The first step in analyzing multivariate data is computing the mean vector and the variance-covariance matrix.\n",
    "\n",
    "The sample mean is a vector each of whose elements is the sample mean of one of the random variables – that is, each of whose elements is the arithmetic average of the observed values of one of the variables.\n",
    "\n",
    "![img](https://i.imgur.com/XG2Yag7.png)\n",
    "\n",
    "For example, take the following two vectors\n",
    "\n",
    "x1 = [2.2, 4.2, …]\n",
    "\n",
    "x2 = [1.2, 3.2, …]\n",
    "\n",
    "So the formulae for calculating `x_mean = 1/2(x1+x2)`\n",
    "\n",
    "= 0.5* [3.4, 7.4, ..]\n",
    "\n",
    "= [1.7,3.7, .. ]\n",
    "\n",
    "#### So we just summed up elements at ith index of the first array and the corresponding index of the second array.\n",
    "\n",
    "#### That also means, that we can say every array can be considered as a vector with each of its indices as one of the dimensions.\n",
    "\n",
    "## Covariance Matrix\n",
    "\n",
    "Covariances Matrix consists of covariances between each pair of variables in the other matrix positions.\n",
    "\n",
    "First a quick look at the difference between covariance and variance. Variance measures the variation of a single random variable (like the height of a person in a population), whereas covariance is a measure of how much two random variables vary together (like the height of a person and the weight of a person in a population). The formula for variance is given by\n",
    "\n",
    "![img](https://i.imgur.com/CnnSCxv.png)\n",
    "\n",
    "### Definition of Co-Variance for 2 scaler values from [Wikipedia](https://en.wikipedia.org/wiki/Covariance)\n",
    "\n",
    "For two jointly distributed real-valued random variables X and Y with finite second moments, the covariance is defined as the expected value (or mean) of the product of their deviations from their individual expected values\n",
    "\n",
    "\n",
    "![img](https://i.imgur.com/jE6qkN7.png)\n",
    "\n",
    "where E[X] is the expected value of X, also known as the mean of X.\n",
    "\n",
    "\n",
    "### Definition of Co-Variance for Matrix form of data - i.e. where the X (as defined in the above formulae is not a Scaler but is a Vector )\n",
    "\n",
    "![img](https://i.imgur.com/eS35C7X.png)\n",
    "\n",
    "#### and  very importantly the transpose appears in this case (for the Matrix form ) because we are multiplying two vectors.\n",
    "\n",
    "#### In another form the formulae for the Covariance Matrix is\n",
    "\n",
    "![img](https://i.imgur.com/AZ10i59.png)\n",
    "\n",
    "Stressing the last point from above if the columns of the original Matrix are standardized, meaning making the column vectors such that the mean is zero and Standard Deviation is 1 the above formulate for Covariance of a Matrix becomes simply\n",
    "\n",
    "#### XX^T/(n-1)\n",
    "\n",
    "## Matrix Multiplication Basics - Explanation on why we are using the transpose of the second element in calculating Covariance Matrix\n",
    "\n",
    "### Rules of matrix multiplication\n",
    "\n",
    "#### a⋅b = a^Tb\n",
    "\n",
    "On the other hand, **b⋅a≠baT**, which is an n×n matrix. The correct expression is\n",
    "\n",
    "#### b⋅a = b^Ta\n",
    "\n",
    "![img](https://i.imgur.com/ZrW2xUy.png)\n",
    "\n",
    "We are indeed taking dot products here. Note, though, that a is a column vector, but a^T is a row vector.\n",
    "\n",
    "By the rules of matrix multiplication, a^Ta and a^Tb result in a 1×1 matrix, which is equivalent to a scalar, while aa^T produces an n×n matrix:\n",
    "\n",
    "![img](https://i.imgur.com/6xjyH0y.png)\n",
    "\n",
    "[Source](https://math.stackexchange.com/a/1853890/517433)\n",
    "\n",
    "Lets see a simple example of a 2*1 Matrix to understand the above vector multiplications and using the Transformed vector for the second element in the above formulae.\n",
    "\n",
    "![img](https://i.imgur.com/KVAIo6b.png)\n",
    "\n",
    "#### [To Read further on Dot product in matrix notation](https://mathinsight.org/dot_product_matrix_notation)\n",
    "\n",
    "![img](https://i.imgur.com/sIBKvlv.png)\n",
    "\n",
    "\n",
    "### Note on using the unbiased estimator (1/n-1) instead of (1/n)\n",
    "\n",
    "This is termed as Bessel's correction. Think of it this way. When we calculate the sample standard deviation from a sample of n values, we are using the sample mean already calculated from that same sample of n values. The calculated sample mean has already \"used up\" one of the \"degrees of freedom of variability\"(which is the mean itself) that is available in the sample. Only n-1 degrees of freedom of variability are left for the calculation of the sample standard deviation.\n",
    "\n",
    "For further details on Bessel's Correction read the following\n",
    "\n",
    "- http://vortex.ihrc.fiu.edu/MET4570/members/Lectures/Lect05/m10divideby_nminus1.pdf\n",
    "- https://math.stackexchange.com/questions/61251/intuitive-explanation-of-bessels-correction\n",
    "\n",
    "---\n",
    "\n",
    "Let’s start with the below matrix.\n",
    "\n",
    "```python\n",
    "A = [[1, 3, 5],\n",
    "       [5, 4, 1],\n",
    "       [3, 8, 6]]\n",
    "```\n",
    "\n",
    "That is our original dataframe is of the below form.\n",
    "\n",
    "![img](https://i.imgur.com/UMS11Dc.png)\n",
    "\n",
    "### We will calculate the covariance between the first and the third column vectors (x1 and x3 features):\n",
    "\n",
    "![img](https://i.imgur.com/J2A8cos.png)\n",
    "\n",
    "![img](https://i.imgur.com/BZbzA1m.png[/img])\n",
    "\n",
    "So, that’s the value (-2.67 ) of the covariance matrix.\n",
    "\n",
    "Now use numpy to do build the full Covariance Matrix. Importantly, here I want NumPy to use the columns as vectors, so the parameter `rowvar=False` has to be used. Also, `bias=True` divides by n and not by n-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3 5]\n",
      " [5 4 1]\n",
      " [3 8 6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.66666667,  0.66666667, -2.66666667],\n",
       "       [ 0.66666667,  4.66666667,  2.33333333],\n",
       "       [-2.66666667,  2.33333333,  4.66666667]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[1, 3, 5], [5, 4, 1], [3, 8, 6]])\n",
    "# A = np.array([[ 3, 5], [4, 1], [8, 6]])\n",
    "print(A)\n",
    "\n",
    "# array([[1, 3, 5],\n",
    "#        [5, 4, 1],\n",
    "#        [3, 8, 6]])\n",
    "\n",
    "cov_matrix = np.cov(A, rowvar=False, bias=True )\n",
    "cov_matrix\n",
    "\n",
    "# array([[ 2.66666667, 0.66666667, -2.66666667],\n",
    "#        [ 0.66666667, 4.66666667, 2.33333333],\n",
    "#        [-2.66666667, 2.33333333, 4.66666667]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### The diagonal elements of this matrix are the variances of the variables, and the off-diagonal elements are the covariances between the variables.\n",
    "\n",
    "In below image all green highlighted are the variances and the rest (red highlighted) are the co-variances\n",
    "\n",
    "![img](https://i.imgur.com/8PJ5lMV.png)\n",
    "\n",
    "### Note, the Dimension of the Co-Variance Matrix will always be a Square Matrix -\n",
    "\n",
    "A Square Matrix is one with same number of rows and columns). Because they capture the covariance between any Variable and any other Variable. So we need all the Variables (Features in a dataset) on the columns and also all the variables on the rows in the resultant Co-Variance Matrix. Thats why in the above image I have put all of x1, x2, x3 in both the columns and rows.\n",
    "\n",
    "#### So, m-dimensional data will result in mxm covariance matrix\n",
    "\n",
    "Basically for three variables (or 3 dimensions of a dataset or 3 features of a dataset) X, Y and Z we will get the Covariance Matrix as following\n",
    "\n",
    "![img](https://i.imgur.com/AAq7ZC1.png)\n",
    "\n",
    "![img](https://i.imgur.com/j0vnWlR.png)\n",
    "\n",
    "Thus,\n",
    "\n",
    "- 2.66666667 is the variance of the x1 variable,\n",
    "\n",
    "- 0.66666667 is the covariance between x1 and x2 variables,\n",
    "\n",
    "- -2.66666667 is the covariance between x1 and x3 variables,\n",
    "\n",
    "- 4.66666667 is the variance of x2,\n",
    "\n",
    "- 2.33333333 is the covariance between x2 and x3 and\n",
    "\n",
    "- 4.66666667 is the variance of x3.\n",
    "\n",
    "#### The mean vector is often referred to as the centroid and the variance-covariance matrix as the dispersion or dispersion matrix. Also, the terms variance-covariance matrix and covariance matrix are used interchangeably.\n",
    "\n",
    "## Properties of Co-Variance Matrix\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Principal Component Analysis\n",
    "\n",
    "PCA stands for Principal Component Analysis. Here is what Wikipedia says about PCA.\n",
    "\n",
    "\"Given a collection of points in two, three, or higher dimensional space, a “best fitting” line can be defined as one that minimizes the average squared distance from a point to the line. The next best-fitting line can be similarly chosen from directions perpendicular to the first. Repeating this process yields an orthogonal basis in which different individual dimensions of the data are uncorrelated. These basis vectors are called principal components, and several related procedures principal component analysis (PCA).\"\n",
    "\n",
    "So, PCA means extracting the components (features) which are most effective in forecasting the target variable in a dataset, and discarding the redundant features. Thus, we calculate the Principal Components to achieve Dimensionality reduction.\n",
    "\n",
    "PCA ( aka Karhunen–Loève transform in Mathematics ) is among the oldest and most widely used methods for dimensionality reduction [105]. The assumption underlying PCA, as well as any dimensionality reduction technique, is that the observed data are generated by a system or process that is driven by a (relatively) small number of latent (not directly observed) variables. The goal is to learn this latent structure.\n",
    "\n",
    "Here's a visual representation of PCA being applied to a simple 2-D data set (left, in blue)\n",
    "\n",
    "![img](https://i.imgur.com/lKK6Gts.png)\n",
    "\n",
    "As we can see above dimension reduction via PCA retains much of the structure of the original data. The ideal subspace is shown in black in the left panel, and the data projected onto this subspace is shown in blue on the right. (bottom panels)\n",
    "\n",
    "Before using PCA, we must standardize our dataset. The method won’t work if we have entirely different scales for our data. Standardization helps to avoid biased outcomes.\n",
    "\n",
    "For example, suppose there are three variables – v1 in a 1-10 range, v2 in a 10-100 range, and v3 in a 100,000 to 1,000,000 range. If we just go ahead and compute an output using these predictors, we’ll get a heavily biased result because the third variable will have a disproportionately large impact on the output value.\n",
    "\n",
    "#### Step-1 So, Before applying PCA, we must ensure that all our attributes (dimensions) are centered around zero and have a standard deviation of 1. So, the first thing we do is calculate the means for all of them. Then, we center the values in each column (to do so we subtract the mean column value) and calculate the covariance matrix for the resulting centered matrix.\n",
    "\n",
    "Now, we can calculate our first principal component.\n",
    "\n",
    "#### Step-2 - Next, we utilize one of the methods for breaking up matrices (eigendecomposition or singular value decomposition) to turn our matrix into a list of eigenvectors that will define directions of axes in the new subspace and eigenvalues that will show the magnitude of those directions.\n",
    "\n",
    "We sort the vectors by their eigenvalues in decreasing order and thus get a ranking of the principal components.\n",
    "\n",
    "#### Step-3 - Now finally, we can ditch some of the dimensions and project our dataset into a reduced subspace without losing important information about the original dataset. And since new axes capture the maximum variance in the data, any clustering algorithm we’re going to apply next will have an easier time grouping the data instances.\n",
    "\n",
    "Lets look at it visually,\n",
    "\n",
    "Imagine this is our dataset that we’re trying to cluster; we only have two dimensions.\n",
    "\n",
    "![img](https://i.imgur.com/CUWkBzi.png)\n",
    "\n",
    "If we project the data onto the horizontal axis (our attribute 1) we won’t see much spread; it will show a nearly equal distribution of the observations.\n",
    "\n",
    "![img](https://i.imgur.com/s5O8hYn.png)\n",
    "\n",
    "Attribute 2, obviously, isn’t hugely helpful either.\n",
    "\n",
    "![img](https://i.imgur.com/6VnyFk7.png)\n",
    "\n",
    "As the data points in this case are spreading diagonally so we need a new line that would better capture this.\n",
    "\n",
    "![img](https://i.imgur.com/B9MtmIg.png)\n",
    "\n",
    "#### This axis is our first Principle Component (PC) – a new direction (we can imagine it to be an attribute) that maximizes the variance of the data (and thus the clusters become much more obvious.) Besides maximizing the spread, this first PC sits through the direction in the data that minimizes the distances between all the points and the new axis.\n",
    "\n",
    "#### The second PC must represent the second maximum amount of variance; it’s going to be a line that’s orthogonal to our first axis.\n",
    "\n",
    "![img](https://i.imgur.com/ocDf36C.png)\n",
    "\n",
    "Because of the underlying PCA-Math being based on eigenvectors and eigenvalues, new principal components will always come out orthogonal to the ones before them. **Eigenvectors**, simply, are vectors that aren’t knocked off of their span by a linear transformation; they can hold on to their original direction while being stretched, shrunk or reversed. Eigenvalues are factors by which these special vectors are scaled.\n",
    "\n",
    "\n",
    "\n",
    "## Key concept of Preserving the Variance by Projection of Data\n",
    "\n",
    "#### Before we can project the training set onto a lower-dimensional hyperplane, we first need to choose the right hyperplane. For example, in the below figure, a simple 2D dataset is represented on the left, along with three different axes (i.e., 1D hyperplanes). On the right is the result of the projection of the dataset onto each of these axes. As we can see, the projection onto the solid line preserves the maximum variance, while the projection onto the dotted line preserves very little variance and the projection onto the dashed line preserves an intermediate amount of variance.\n",
    "\n",
    "![img](https://i.imgur.com/Fp8kqgJ.png)\n",
    "\n",
    "#### It seems reasonable to select the axis that preserves the maximum amount of variance, as it will most likely lose less information than the other projections. Another way to justify this choice is that it is the axis that minimizes the mean squared distance between the original dataset and its projection onto that axis. This is the basic idea behind PCA.\n",
    "\n",
    "PCA identifies the axis that accounts for the largest amount of variance in the train-ing set. In above figure, it is the solid line. It also finds a second axis, orthogonal to the first one, that accounts for the largest amount of remaining variance. In this 2D example there is no choice: it is the dotted line (i.e. the orthogonal one). If it were a higher-dimensional data‐set, PCA would also find a third axis, orthogonal to both previous axes, and a fourth, a fifth, and so on—as many axes as the number of dimensions in the dataset. The ith axis is called the ith principal component (PC) of the data. In above figure, the first PC is the axis on which vector c1 lies, and the second PC is the axis on which vector c2 lies. In case of 3-D dataset the first two PCs are the orthogonal axes on which the two arrows lie, on the plane, and the third PC is the axis orthogonal to that plane, like in the below figure.\n",
    "\n",
    "![img](https://i.imgur.com/wEHwUMl.png)\n",
    "\n",
    "\n",
    "For each principal component, PCA finds a zero-centered unit vector pointing in the direction of the PC. Since two opposing unit vectors lie on the same axis, the direction of the unit vectors returned by PCA is not stable: if you perturb the training set slightly and run PCA again, the unit vectors may point in the oppo‐\n",
    "site direction as the original vectors. However, they will generally still lie on the same axes. In some cases, a pair of unit vectors may even rotate or swap (if the variances along these two axes are close), but the plane they define will generally remain the same.\n",
    "\n",
    "\n",
    "## Eigenvalues and Eigenvectors\n",
    "Many problems present themselves in terms of an eigenvalue problem:\n",
    "\n",
    "###  A·v=λ·v\n",
    "\n",
    "In this equation A is an n-by-n matrix, v is a non-zero n-by-1 vector and λ is a scalar (which may be either real or complex).  Any value of λ for which this equation has a solution is known as an eigenvalue of the matrix A.\n",
    "\n",
    "To begin, let $v$ be a vector (shown as a point) and $A$ be a matrix with columns $a_1$ and $a_2$ (shown as arrows). If we multiply $v$ by $A$, then $A$ sends $v$ to a new vector $Av$.\n",
    "\n",
    "![img](https://i.imgur.com/xBy93Vd.png)\n",
    "\n",
    "\n",
    "If you can draw a line through the three points $(0,0)$, $v$ and $Av$, then $Av$ is just $v$ multiplied by a number $\\lambda$; that is, $Av = \\lambda v$. In this case, we call $\\lambda$ an eigenvalue and $v$ an eigenvector.\n",
    "\n",
    "This means that the linear transformation A on vector **v** is completely defined by λ.\n",
    "\n",
    "In the linear algebra literature and software, it is often a convention that eigenvalues are sorted in descending order, so that the largest\n",
    "eigenvalue and associated eigenvector are called the first eigenvalue and its associated eigenvector, and the second largest called the second eigenvalue and its associated eigenvector, and so on.\n",
    "\n",
    "An eigenvector is a vector whose direction remains unchanged when a linear transformation is applied to it.\n",
    "\n",
    "\n",
    "### Usefullness of Eigenvalues and Eigenvectors\n",
    "\n",
    "If you keep multiplying $v$ by $A$, you get a sequence ${ v, Av, A^2v,}$ etc. Eigenspaces attract that sequence and eigenvalues tell you whether it ends up at $(0,0)$ or far away. Therefore, eigenvectors/values tell us about systems that evolve step-by-step.\n",
    "\n",
    "![img]https://i.imgur.com/1hXen7o.png[/img]\n",
    "\n",
    "### Properties of eigenvalues and eigenvectors\n",
    "\n",
    "- Let A be a $K\times K$ square matrix. A scalar $lambda $ is an eigenvalue of A if and only if it is an eigenvalue of $A^{intercal }$.\n",
    "\n",
    "-  For symmetric matrix, the eigenvectors are always orthogonal\n",
    "\n",
    "In general, for any matrix, the eigenvectors are NOT always orthogonal. But for a special type of matrix, symmetric matrix, the eigenvalues are always real and the corresponding eigenvectors are always orthogonal.\n",
    "\n",
    "For any matrix M with n rows and m columns, M multiplies with its transpose, either M*M' or M'M, results in a symmetric matrix, so for this symmetric matrix, the eigenvectors are always orthogonal.\n",
    "\n",
    "In the application of PCA, a dataset of n samples with m features is usually represented in a n* m matrix D. The variance and covariance among those m features can be represented by a m*m matrix D'*D, which is symmetric (numbers on the diagonal represent the variance of each single feature, and the number on row i column j represents the covariance between feature i and j). The PCA is applied on this symmetric matrix, so the eigenvectors are guaranteed to be orthogonal.\n",
    "\n",
    "\n",
    "#### During Steps PCA calculation steps we need the Eigenvectors as below\n",
    "\n",
    "- Step-A => Standardize the Columns\n",
    "\n",
    "- Step-B => Build the Covariance-Matrix S which is $(X^T * X)$\n",
    "\n",
    "- Step-C => Get the EigneVectors of this Covariance-Matrix S. These will be sorted from largest λ and downwards.\n",
    "\n",
    "- Step-D => And my **u1** is the Eigenvector **v1** which is the largest Eigenvector corresponding to the largest Eigenvalue\n",
    "\n",
    "![img](https://i.imgur.com/grOzxqv.png)\n",
    "\n",
    "### Significance of Eigenvectors for a 10-Dimensional dataset - Below is a Geometric interpretation\n",
    "\n",
    "#### The highest Eigenvector v1 (corresponding to the max lambda i.e. max Eigenvalue ) - is the vector representing the direction with the highest Variance (Principal Component). Then v2 is the direction with the second highest variance\n",
    "\n",
    "![img](https://i.imgur.com/Y4NLf1I.png)\n",
    "\n",
    "[Source-AppliedAI Course Youtube Video](https://www.youtube.com/watch?v=b0DHvcdIStE&feature=emb_logo&ab_channel=AppliedAICourse)\n",
    "\n",
    "\n",
    "### Significance of Eigenvalues (Lambdas) for a 2-Dimensional dataset - a Geometric interpretation\n",
    "\n",
    "#### A particular Lambda in a particular direction / Summation of All Lambdas - tells me the % of the Variances retained in that direction.\n",
    "\n",
    "![img](https://i.imgur.com/WCX268w.png)\n",
    "\n",
    "So in the above picture, for the middle graph (representing a 2-d dataset) - lambda-1 retains (3/4)th or 75% of the variances.\n",
    "\n",
    "[Source-AppliedAI Course Youtube Video](https://www.youtube.com/watch?v=b0DHvcdIStE&feature=emb_logo&ab_channel=AppliedAICourse)\n",
    "\n",
    "\n",
    "#### That means the dot product of two eigenvectors of a Symmetric Matrix is always zero\n",
    "\n",
    "![img](https://i.imgur.com/lzlNX4Y.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## The Mathematics of PCA\n",
    "\n",
    "### Under the Maximum Variance Formulae\n",
    "\n",
    "### First here is the what the Projection Vector looks like geometrically, for point x_i and also the mathematical formula of point x_i on the Principle Component Vector u_1\n",
    "\n",
    "![img](https://i.imgur.com/WirAvCo.png)\n",
    "\n",
    "In English the here the Math Objective of the Problem - assuming **u1** represents the Principle component Vector\n",
    "\n",
    "### Find u1, such that the the Variance of points x_i ( i going from 1 to n) projected onto u1 is maximized\n",
    "\n",
    "Mathematically\n",
    "\n",
    "![img](https://i.imgur.com/E5mMcaD.png)\n",
    "\n",
    "A slightly more formally\n",
    "\n",
    "![img](https://i.imgur.com/teprtEQ.png)\n",
    "\n",
    "[Source of above Image](https://www.cs.toronto.edu/~urtasun/courses/CSC411/tutorial8.pdf)\n",
    "\n",
    "#### But note our original dataset Matrix has been centered or column-standardized, hence\n",
    "\n",
    "![img](https://i.imgur.com/yAj6dE6.png)\n",
    "\n",
    "### So the final Objective function becomes\n",
    "\n",
    "![img](https://i.imgur.com/ByyY9Wr.png)\n",
    "\n",
    "### So extending the above, our absolute final target is to find u_1 for which the above variance will be maximized.\n",
    "\n",
    "---\n",
    "\n",
    "Don’t confuse PCA with linear regression. PCA is to minimize the distance of the orthogonal projection, while linear regression is to minimize the distance on the y-axis.\n",
    "\n",
    "![img](https://i.imgur.com/x3cFb0o.png)\n",
    "\n",
    "[Source of above image](http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/)\n",
    "\n",
    "## Why is minimizing squared residuals equivalent to maximizing variance?\n",
    "\n",
    "To ans this question I could not find a better explanations than has been described **[here in this blog](http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/)**\n",
    "\n",
    "![img](https://i.imgur.com/w7gM6lu.png)\n",
    "\n",
    " Where **w** is the projection of each datapoint onto the top principal component and the vector **c**  is the top principal component\n",
    "\n",
    "## Important Note - PCA assumes that the dataset is centered around the origin.\n",
    "\n",
    "As we will see, Scikit-Learn’s PCA classes take care of centering the data for you. If you implement PCA yourself, or if you use other libraries, don’t forget to center the data first\n",
    "\n",
    "## Now I will Calculate the PCA quite manually using the steps described earlier, which are\n",
    "\n",
    "- Step-A => Standardize the Columns\n",
    "\n",
    "- Step-B => Build the Covariance-Matrix S which is $(X^T * X)$\n",
    "\n",
    "- Step-C => Get the EigneVectors of this Covariance-Matrix S. These will be sorted from largest λ and downwards.\n",
    "\n",
    "- Step-D => And my **u1** is the Eigenvector **v1** which is the largest Eigenvector corresponding to the largest Eigenvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "sb.set_style(\"dark\")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# In my local Machine, I ah only taking fewer rows for faster local training and development\n",
    "# In Kaggle comment-out below line\n",
    "train_df_org = pd.read_csv('kaggle/input/digit-recognizer/train.csv',  nrows=100)\n",
    "\n",
    "# Instead, uncomment the below one, where the whole dataset is taken for traiing\n",
    "# train_df_org = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\n",
    "\n",
    "# test_df_org = np.genfromtxt('kaggle/input/digit-recognizer/test.csv', delimiter=',', skip_header=1)\n",
    "# sample_submission_df = pd.read_csv('kaggle/input/digit-recognizer/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 785)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_df_org.shape, test_df_org.shape, sample_submission_df.shape\n",
    "train_df_org.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In below the labels variable contains the labels and then from the 'train_df_org' (which contains all the actual image pixel) I will drop 'label' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = train_df_org['label']\n",
    "labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_org.drop(['label'], axis=1, inplace=True)\n",
    "train_df_org.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784)\n"
     ]
    }
   ],
   "source": [
    "# Data-preprocessing: Standardizing the train dataframe\n",
    "# (x - mean) / s.d\n",
    "\n",
    "standardized_data = StandardScaler().fit_transform(train_df_org)\n",
    "print(standardized_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Co-variance matrix =  (784, 784)\n"
     ]
    }
   ],
   "source": [
    "# Now build the c-variance matrix which is : A^T * A\n",
    "sample_data = standardized_data\n",
    "\n",
    "# Matrix multiplication with numpy\n",
    "covariance_matrix = np.matmul(sample_data.T, sample_data)\n",
    "\n",
    "# As the sample_data has 784 columns, so the co-variance matrix shape\n",
    "# should be 784 * 784\n",
    "print('Shape of Co-variance matrix = ', covariance_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-C => Get the EigneVectors of this Covariance-Matrix S for projecting onto a 2-D space. These Eigenvectors will be sorted by the value of λ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Eigenvectors  (784, 2)\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import eigh\n",
    "\n",
    "# eigh function will give me the eigenvalues in ascending order with zero-index\n",
    "# subset_by_index is an optional and iterable parameter\n",
    "# If provided, this two-element iterable defines the start and the end indices\n",
    "# of the desired eigenvalues (ascending order and 0-indexed).\n",
    "# To return only the second smallest to fifth smallest eigenvalues, [1, 4] is used.\n",
    "# [n-3, n-1] returns the largest three.\n",
    "# Only available with “evr”, “evx”, and “gvx” drivers. The entries are directly converted to integers via int().\n",
    "\n",
    "# Needed to comment out below line containing 'subset_by_index' parameter, as it would fail\n",
    "# in Kaggle, probably because of the driver requirement.\n",
    "# eigenvalues, eigenvectors = eigh(covariance_matrix, subset_by_index=(782, 783) )\n",
    "\n",
    "# instead using 'eigvals' which is deprecated per doc\n",
    "eigenvalues, eigenvectors = eigh(covariance_matrix, eigvals=(782, 783) )\n",
    "print('Shape of Eigenvectors ', eigenvectors.shape )\n",
    "\n",
    "# In above I am only passing 782 and 783 as eigvalues becuase\n",
    "# Our target is to reduce the 784-dimension data to 2-dimension data and plot it.\n",
    "# So we need the top 2 eigvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So you have the principal components. They are eigenvectors of the _covariance_ matrix $X^T X$.\n",
    "\n",
    "A way to retrieve the eigenvalues from there is to apply this matrix to each principal components and project the results onto the component.\n",
    "Let v_1  be the first principal component and lambda_1 the associated eigenvalue. We have:\n",
    "[![eq][1]][1] and thus:\n",
    "[![eq2][2]][2] since [![eq3][3]][3]. (x, y)  the scalar product of vectors x and y.\n",
    "\n",
    "[1]: http://i.stack.imgur.com/6OApA.gif\n",
    "[2]: http://i.stack.imgur.com/iCZcI.gif\n",
    "[3]: http://i.stack.imgur.com/8pGd1.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# now converting the eigenvectors into (2, d) shape for eaze of computation further ahead\n",
    "eigenvectors = eigenvectors.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " #### eigenvectors[0] represents the eigenvector corresponding to 1st Principal Component\n",
    " #### eigenvectors[1] represents the eigenvector corresponding to 2nd Principal Component\n",
    "\n",
    " Now finally our target which was to reduce the 784-dimension data to 2-dimension data and plot it.\n",
    "\n",
    "Now I need to project the original data sample on the plane formed by two principle eigenvectors by vector-vector multiplication.\n",
    "\n",
    "### Simplest Formulate to Refer All time\n",
    "\n",
    "https://users.math.msu.edu/users/gnagy/teaching/11-fall/mth234/L03-234.pdf\n",
    "\n",
    "![img](https://i.imgur.com/enenSdD.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projected_vec = np.matmul(eigenvectors, sample_data.T)\n",
    "projected_vec.shape\n",
    "# Now we will see this is a 2-dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now I will use vstack() function to Stack arrays in sequence vertically (row wise). This function makes most sense for arrays with up to 3 dimensions. For instance, for pixel-data with a height (first axis), width (second axis), and r/g/b channels (third axis).\n",
    "\n",
    "```python\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([2, 3, 4])\n",
    "np.vstack((a,b))\n",
    "array([[1, 2, 3],\n",
    "       [2, 3, 4]])\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# appending label to the 2-d projected data\n",
    "projected_vec = np.vstack((projected_vec, labels)).T\n",
    "projected_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The projected_vec at this point is still in array form and so we can not apply\n",
    "# head() function to it directly and also for plotting purpose, I need to make a dataframe from it.\n",
    "# creating a new dataframe from plotting the labeled points\n",
    "pca_dataframe = pd.DataFrame(data=projected_vec, columns=('1st_principal_comp', \"2nd_principal_comp\", 'labels') )\n",
    "pca_dataframe.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So in above each row is x_i which was originally 784 dimensions and now they are in 2-dimensions by applying PCA."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sb.FacetGrid(pca_dataframe, hue='labels', size=6).map(plt.scatter, '1st_principal_comp', \"2nd_principal_comp\").add_legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And we can see some degree of classifications among the digits. For example, all zeros are on the top left corner. All the sixes (oranges) are towards the bottom between 0 and 4 on the x-axis.\n",
    "\n",
    "## And\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}