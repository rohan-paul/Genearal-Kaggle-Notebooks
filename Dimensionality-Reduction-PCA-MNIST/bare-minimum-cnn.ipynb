{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
      "0       0       0       0       0       0       0       0       0       0   \n",
      "1       0       0       0       0       0       0       0       0       0   \n",
      "2       0       0       0       0       0       0       0       0       0   \n",
      "3       0       0       0       0       0       0       0       0       0   \n",
      "4       0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
      "0       0  ...         0         0         0         0         0         0   \n",
      "1       0  ...         0         0         0         0         0         0   \n",
      "2       0  ...         0         0         0         0         0         0   \n",
      "3       0  ...         0         0         0         0         0         0   \n",
      "4       0  ...         0         0         0         0         0         0   \n",
      "\n",
      "   pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0  \n",
      "1         0         0         0         0  \n",
      "2         0         0         0         0  \n",
      "3         0         0         0         0  \n",
      "4         0         0         0         0  \n",
      "\n",
      "[5 rows x 784 columns]\n",
      "0    1\n",
      "1    0\n",
      "2    1\n",
      "3    4\n",
      "4    0\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "sb.set_style(\"dark\")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_df_org = pd.read_csv('kaggle/input/digit-recognizer/train.csv',  nrows=300)\n",
    "\n",
    "mnist_train_data = train_df_org.loc[:, \"pixel0\":]\n",
    "print(mnist_train_data.head())\n",
    "# print(mnist_train_data.shape) # (1000, 784)\n",
    "\n",
    "mnist_train_label = train_df_org.loc[:, \"label\"]\n",
    "print(mnist_train_label.head())\n",
    "# print(mnist_train_label.shape) # (1000,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Lambda, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, AvgPool2D\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 28, 28, 1) (300,)\n"
     ]
    }
   ],
   "source": [
    "# Converting dataframe into arrays\n",
    "mnist_train_data = np.array(mnist_train_data)\n",
    "mnist_train_label = np.array(mnist_train_label)\n",
    "# Reshaping the input shapes to get it in the shape which the model expects to recieve later.\n",
    "mnist_train_data = mnist_train_data.reshape(mnist_train_data.shape[0], 28, 28, 1)\n",
    "print(mnist_train_data.shape, mnist_train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of ytrain after encoding:  (300, 10)\n"
     ]
    }
   ],
   "source": [
    "nclasses = mnist_train_label.max() - mnist_train_label.min() + 1\n",
    "mnist_train_label = to_categorical(mnist_train_label, num_classes = nclasses)\n",
    "print(\"Shape of ytrain after encoding: \", mnist_train_label.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape=(28, 28, 1)):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, kernel_size = 3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size = 4, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_model(model, optimizer='adam', loss='categorical_crossentropy'):\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "def train_model(model, train, test, epochs, split):\n",
    "    history = model.fit(train, test, shuffle=True, epochs=epochs, validation_split=split)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2/8 [======>.......................] - ETA: 0s - loss: 3.3702 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0170s vs `on_train_batch_end` time: 0.0956s). Check your callbacks.\n",
      "8/8 [==============================] - 3s 337ms/step - loss: 3.1004 - accuracy: 0.1750 - val_loss: 2.1602 - val_accuracy: 0.1667\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 1.9830 - accuracy: 0.3500 - val_loss: 2.0061 - val_accuracy: 0.3000\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 1.3320 - accuracy: 0.5583 - val_loss: 1.8615 - val_accuracy: 0.3333\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.9733 - accuracy: 0.6958 - val_loss: 1.7283 - val_accuracy: 0.4333\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.6952 - accuracy: 0.7708 - val_loss: 1.5647 - val_accuracy: 0.5333\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.4642 - accuracy: 0.8500 - val_loss: 1.3931 - val_accuracy: 0.5667\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.4191 - accuracy: 0.8917 - val_loss: 1.2279 - val_accuracy: 0.6000\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.3760 - accuracy: 0.8792 - val_loss: 1.1073 - val_accuracy: 0.6333\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.2783 - accuracy: 0.9208 - val_loss: 0.9976 - val_accuracy: 0.6833\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.1886 - accuracy: 0.9667 - val_loss: 0.8315 - val_accuracy: 0.7500\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.1991 - accuracy: 0.9458 - val_loss: 0.7076 - val_accuracy: 0.8333\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.1824 - accuracy: 0.9625 - val_loss: 0.6417 - val_accuracy: 0.8333\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.1214 - accuracy: 0.9708 - val_loss: 0.5784 - val_accuracy: 0.8667\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.1381 - accuracy: 0.9542 - val_loss: 0.5276 - val_accuracy: 0.8667\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.0914 - accuracy: 0.9917 - val_loss: 0.4988 - val_accuracy: 0.8667\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.0963 - accuracy: 0.9833 - val_loss: 0.4912 - val_accuracy: 0.8667\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.0776 - accuracy: 0.9875 - val_loss: 0.4552 - val_accuracy: 0.8667\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.0758 - accuracy: 0.9875 - val_loss: 0.4209 - val_accuracy: 0.8667\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.0694 - accuracy: 0.9875 - val_loss: 0.3918 - val_accuracy: 0.8833\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.0486 - accuracy: 0.9917 - val_loss: 0.3676 - val_accuracy: 0.8833\n"
     ]
    }
   ],
   "source": [
    "cnn_model = build_model((28, 28, 1))\n",
    "compile_model(cnn_model, 'adam', 'categorical_crossentropy')\n",
    "\n",
    "# train the model for as many epochs as you want but I found training it above 80 will not help us and eventually increase overfitting.\n",
    "model_history = train_model(cnn_model, mnist_train_data, mnist_train_label, 4, 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}